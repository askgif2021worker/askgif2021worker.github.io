{"id":1685,"title":"How to Design the Twitter timeline and search","imageUrl":"icon_4cffb0b1-c317-4027-9ff2-188ac3240e74.jpg","dateCreated":"2020-11-20T06:38:46.064Z","dateModified":"2020-11-20T06:39:08.542Z","contributedBy":"sumitc91","content":"<p>Design the Facebook feed and Design Facebook search are similar questions.</p>\n<p>Step 1: Outline use cases and constraints<br />Gather requirements and scope the problem. Ask questions to clarify use cases and constraints. Discuss assumptions.</p>\n<p>Without an interviewer to address clarifying questions, we'll define some use cases and constraints.</p>\n<p>Use cases<br />We'll scope the problem to handle only the following use cases<br />User posts a tweet<br />Service pushes tweets to followers, sending push notifications and emails<br />User views the user timeline (activity from the user)<br />User views the home timeline (activity from people the user is following)<br />User searches keywords<br />Service has high availability</p>\n<p><br />Out of scope<br />Service pushes tweets to the Twitter Firehose and other streams<br />Service strips out tweets based on users' visibility settings<br />Hide @reply if the user is not also following the person being replied to<br />Respect 'hide retweets' setting<br />Analytics</p>\n<p><br />Constraints and assumptions<br />State assumptions<br />General</p>\n<p>Traffic is not evenly distributed<br />Posting a tweet should be fast<br />Fanning out a tweet to all of your followers should be fast, unless you have millions of followers<br />100 million active users<br />500 million tweets per day or 15 billion tweets per month<br />Each tweet averages a fanout of 10 deliveries<br />5 billion total tweets delivered on fanout per day<br />150 billion tweets delivered on fanout per month<br />250 billion read requests per month<br />10 billion searches per month<br />Timeline</p>\n<p>Viewing the timeline should be fast<br />Twitter is more read heavy than write heavy<br />Optimize for fast reads of tweets<br />Ingesting tweets is write heavy<br />Search</p>\n<p>Searching should be fast<br />Search is read-heavy<br />Calculate usage<br />Clarify with your interviewer if you should run back-of-the-envelope usage calculations.</p>\n<p>Size per tweet:<br />tweet_id - 8 bytes<br />user_id - 32 bytes<br />text - 140 bytes<br />media - 10 KB average<br />Total: ~10 KB<br />150 TB of new tweet content per month<br />10 KB per tweet * 500 million tweets per day * 30 days per month<br />5.4 PB of new tweet content in 3 years<br />100 thousand read requests per second<br />250 billion read requests per month * (400 requests per second / 1 billion requests per month)<br />6,000 tweets per second<br />15 billion tweets per month * (400 requests per second / 1 billion requests per month)<br />60 thousand tweets delivered on fanout per second<br />150 billion tweets delivered on fanout per month * (400 requests per second / 1 billion requests per month)<br />4,000 search requests per second<br />10 billion searches per month * (400 requests per second / 1 billion requests per month)<br />Handy conversion guide:</p>\n<p>2.5 million seconds per month<br />1 request per second = 2.5 million requests per month<br />40 requests per second = 100 million requests per month<br />400 requests per second = 1 billion requests per month<br />Step 2: Create a high level design<br />Outline a high level design with all important components.</p>\n<p>Imgur</p>\n<p>Step 3: Design core components<br />Dive into details for each core component.</p>\n<p>Use case: User posts a tweet<br />We could store the user's own tweets to populate the user timeline (activity from the user) in a relational database. We should discuss the use cases and tradeoffs between choosing SQL or NoSQL.</p>\n<p>Delivering tweets and building the home timeline (activity from people the user is following) is trickier. Fanning out tweets to all followers (60 thousand tweets delivered on fanout per second) will overload a traditional relational database. We'll probably want to choose a data store with fast writes such as a NoSQL database or Memory Cache. Reading 1 MB sequentially from memory takes about 250 microseconds, while reading from SSD takes 4x and from disk takes 80x longer.1</p>\n<p>We could store media such as photos or videos on an Object Store.</p>\n<p>The Client posts a tweet to the Web Server, running as a reverse proxy<br />The Web Server forwards the request to the Write API server<br />The Write API stores the tweet in the user's timeline on a SQL database<br />The Write API contacts the Fan Out Service, which does the following:<br />Queries the User Graph Service to find the user's followers stored in the Memory Cache<br />Stores the tweet in the home timeline of the user's followers in a Memory Cache<br />O(n) operation: 1,000 followers = 1,000 lookups and inserts<br />Stores the tweet in the Search Index Service to enable fast searching<br />Stores media in the Object Store<br />Uses the Notification Service to send out push notifications to followers:<br />Uses a Queue (not pictured) to asynchronously send out notifications<br />Clarify with your interviewer how much code you are expected to write.</p>\n<p>If our Memory Cache is Redis, we could use a native Redis list with the following structure:</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;tweet n+2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tweet n+1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tweet n<br />| 8 bytes &nbsp; 8 bytes &nbsp;1 byte | 8 bytes &nbsp; 8 bytes &nbsp;1 byte | 8 bytes &nbsp; 8 bytes &nbsp;1 byte |<br />| tweet_id &nbsp;user_id &nbsp;meta &nbsp; | tweet_id &nbsp;user_id &nbsp;meta &nbsp; | tweet_id &nbsp;user_id &nbsp;meta &nbsp; |<br />The new tweet would be placed in the Memory Cache, which populates the user's home timeline (activity from people the user is following).</p>\n<p>We'll use a public REST API:</p>\n<p>$ curl -X POST --data '{ \"user_id\": \"123\", \"auth_token\": \"ABC123\", \\<br />&nbsp; &nbsp; \"status\": \"hello world!\", \"media_ids\": \"ABC987\" }' \\<br />&nbsp; &nbsp; https://twitter.com/api/v1/tweet<br />Response:</p>\n<p>{<br />&nbsp; &nbsp; \"created_at\": \"Wed Sep 05 00:37:15 +0000 2012\",<br />&nbsp; &nbsp; \"status\": \"hello world!\",<br />&nbsp; &nbsp; \"tweet_id\": \"987\",<br />&nbsp; &nbsp; \"user_id\": \"123\",<br />&nbsp; &nbsp; ...<br />}<br />For internal communications, we could use Remote Procedure Calls.</p>\n<p>Use case: User views the home timeline<br />The Client posts a home timeline request to the Web Server<br />The Web Server forwards the request to the Read API server<br />The Read API server contacts the Timeline Service, which does the following:<br />Gets the timeline data stored in the Memory Cache, containing tweet ids and user ids - O(1)<br />Queries the Tweet Info Service with a multiget to obtain additional info about the tweet ids - O(n)<br />Queries the User Info Service with a multiget to obtain additional info about the user ids - O(n)<br />REST API:</p>\n<p>$ curl https://twitter.com/api/v1/home_timeline?user_id=123<br />Response:</p>\n<p>{<br />&nbsp; &nbsp; \"user_id\": \"456\",<br />&nbsp; &nbsp; \"tweet_id\": \"123\",<br />&nbsp; &nbsp; \"status\": \"foo\"<br />},<br />{<br />&nbsp; &nbsp; \"user_id\": \"789\",<br />&nbsp; &nbsp; \"tweet_id\": \"456\",<br />&nbsp; &nbsp; \"status\": \"bar\"<br />},<br />{<br />&nbsp; &nbsp; \"user_id\": \"789\",<br />&nbsp; &nbsp; \"tweet_id\": \"579\",<br />&nbsp; &nbsp; \"status\": \"baz\"<br />},<br />Use case: User views the user timeline<br />The Client posts a user timeline request to the Web Server<br />The Web Server forwards the request to the Read API server<br />The Read API retrieves the user timeline from the SQL Database<br />The REST API would be similar to the home timeline, except all tweets would come from the user as opposed to the people the user is following.</p>\n<p>Use case: User searches keywords<br />The Client sends a search request to the Web Server<br />The Web Server forwards the request to the Search API server<br />The Search API contacts the Search Service, which does the following:<br />Parses/tokenizes the input query, determining what needs to be searched<br />Removes markup<br />Breaks up the text into terms<br />Fixes typos<br />Normalizes capitalization<br />Converts the query to use boolean operations<br />Queries the Search Cluster (ie Lucene) for the results:<br />Scatter gathers each server in the cluster to determine if there are any results for the query<br />Merges, ranks, sorts, and returns the results<br />REST API:</p>\n<p>$ curl https://twitter.com/api/v1/search?query=hello+world<br />The response would be similar to that of the home timeline, except for tweets matching the given query.</p>\n<p>Step 4: Scale the design<br />Identify and address bottlenecks, given the constraints.</p>\n<p>Imgur</p>\n<p>Important: Do not simply jump right into the final design from the initial design!</p>\n<p>State you would 1) Benchmark/Load Test, 2) Profile for bottlenecks 3) address bottlenecks while evaluating alternatives and trade-offs, and 4) repeat. See Design a system that scales to millions of users on AWS as a sample on how to iteratively scale the initial design.</p>\n<p>It's important to discuss what bottlenecks you might encounter with the initial design and how you might address each of them. For example, what issues are addressed by adding a Load Balancer with multiple Web Servers? CDN? Master-Slave Replicas? What are the alternatives and Trade-Offs for each?</p>\n<p>We'll introduce some components to complete the design and to address scalability issues. Internal load balancers are not shown to reduce clutter.</p>\n<p>To avoid repeating discussions, refer to the following system design topics for main talking points, tradeoffs, and alternatives:</p>\n<p>DNS<br />CDN<br />Load balancer<br />Horizontal scaling<br />Web server (reverse proxy)<br />API server (application layer)<br />Cache<br />Relational database management system (RDBMS)<br />SQL write master-slave failover<br />Master-slave replication<br />Consistency patterns<br />Availability patterns<br />The Fanout Service is a potential bottleneck. Twitter users with millions of followers could take several minutes to have their tweets go through the fanout process. This could lead to race conditions with @replies to the tweet, which we could mitigate by re-ordering the tweets at serve time.</p>\n<p>We could also avoid fanning out tweets from highly-followed users. Instead, we could search to find tweets for highly-followed users, merge the search results with the user's home timeline results, then re-order the tweets at serve time.</p>\n<p>Additional optimizations include:</p>\n<p>Keep only several hundred tweets for each home timeline in the Memory Cache<br />Keep only active users' home timeline info in the Memory Cache<br />If a user was not previously active in the past 30 days, we could rebuild the timeline from the SQL Database<br />Query the User Graph Service to determine who the user is following<br />Get the tweets from the SQL Database and add them to the Memory Cache<br />Store only a month of tweets in the Tweet Info Service<br />Store only active users in the User Info Service<br />The Search Cluster would likely need to keep the tweets in memory to keep latency low<br />We'll also want to address the bottleneck with the SQL Database.</p>\n<p>Although the Memory Cache should reduce the load on the database, it is unlikely the SQL Read Replicas alone would be enough to handle the cache misses. We'll probably need to employ additional SQL scaling patterns.</p>\n<p>The high volume of writes would overwhelm a single SQL Write Master-Slave, also pointing to a need for additional scaling techniques.</p>\n<p>Federation<br />Sharding<br />Denormalization<br />SQL Tuning<br />We should also consider moving some data to a NoSQL Database.</p>","ampImage":"https://raw.githubusercontent.com/sumitc91/AmpImage/main/Blogs/4cffb0b1-c317-4027-9ff2-188ac3240e74.jpg","subContents":[],"category":{"name":"tutorials","slug":"tutorials"},"slug":"how-to-design-the-twitter-timeline-and-search","tags":[{"name":"design","slug":"design"},{"name":"interview","slug":"interview"},{"name":"system-design","slug":"system-design"},{"name":"twitter","slug":"twitter"}],"recommendations":[{"id":1697,"title":"What is Reverse proxy (web server)","imageUrl":"icon_c9b9941f-ccf0-43b6-8c8d-af837fe0b02d.jpg","dateCreated":"2020-11-20T07:38:40.516Z","dateModified":"2020-11-20T07:38:45.525Z","contributedBy":"sumitc91","content":"A reverse proxy is a web server that centralizes internal services and provides unified interfaces to the public. Requests from clients are forwarded to a server that can fulfill it before the reverse proxy returns the server's response to the client.\nAdditional benefits include:\nIn","ampImage":"https://raw.githubusercontent.com/sumitc91/AmpImage/main/Blogs/c9b9941f-ccf0-43b6-8c8d-af837fe0b02d.jpg","category":{"name":"tutorials","slug":"tutorials"},"tags":[{"name":"interview","slug":"interview"}],"slug":"what-is-reverse-proxy-web-server","total":0},{"id":58,"title":"How to solve Knapsack problem using Dynamic Programming","imageUrl":"b6cbe28d-ef39-4235-aa96-015ea58e9991_knapsack.jpg","dateCreated":"2018-07-31T07:51:18.76Z","dateModified":"2018-07-31T07:51:31.675Z","contributedBy":"AskGif","content":"The knapsack problem or rucksack problem is a problem in combinatorial optimization: Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large a","ampImage":"https://raw.githubusercontent.com/sumitc91/AmpImage/main/Blogs/b6cbe28d-ef39-4235-aa96-015ea58e9991_knapsack.jpg","category":{"name":"coding","slug":"coding"},"tags":[{"name":"recursion","slug":"recursion"},{"name":"interview","slug":"interview"},{"name":"questions","slug":"questions"},{"name":"dynamic-programming","slug":"dynamic-programming"},{"name":"java","slug":"java"}],"slug":"how-to-solve-knapsack-problem-using-dynamic-programming","total":0},{"id":18,"title":"Find Longest Increasing Subsequence","imageUrl":"2f4b2949-decb-492c-a28a-354c2bc38e48_Longest-Increasing-Subsequence-track-the-result.png","dateCreated":"2018-07-12T07:55:42.701Z","dateModified":"2018-08-31T04:38:31.491Z","contributedBy":"AskGif","content":"Finding Longest Increasing Subsequence in an array in N^2 Time complexity.\r\n<span style=\"color: #576871; font-family: OpenSans, Arial, Helvetica, sans-serif;\">The task is to find the length of the longest subsequence in a given array of integers such that all elements of the subsequence ar","ampImage":"https://raw.githubusercontent.com/sumitc91/AmpImage/main/Blogs/2f4b2949-decb-492c-a28a-354c2bc38e48_Longest-Increasing-Subsequence-track-the-result.png","category":{"name":"coding","slug":"coding"},"tags":[{"name":"lis","slug":"lis"},{"name":"flipkart","slug":"flipkart"},{"name":"dynamic-programming","slug":"dynamic-programming"},{"name":"java","slug":"java"},{"name":"interview","slug":"interview"},{"name":"amazon","slug":"amazon"},{"name":"questions","slug":"questions"},{"name":"treebo","slug":"treebo"}],"slug":"find-longest-increasing-subsequence","total":0},{"id":17,"title":"Write a Simple Hello World Program in JAVA.","imageUrl":"f91c0323-afd7-4b3e-aa1f-71c79e9913f2_hello-world-java.png","dateCreated":"2018-07-12T06:44:09.184Z","dateModified":"2018-07-12T06:44:59.673Z","contributedBy":"AskGif","content":"Writing a simple hello world programm in java.&nbsp;Java is a general-purpose computer-programming language that is concurrent, class-based, object-oriented, and specifically designed to have as few implementation dependencies as possible. It is intended to let application developers \"write once,","ampImage":"https://raw.githubusercontent.com/sumitc91/AmpImage/main/Blogs/f91c0323-afd7-4b3e-aa1f-71c79e9913f2_hello-world-java.png","category":{"name":"coding","slug":"coding"},"tags":[{"name":"interview","slug":"interview"},{"name":"java","slug":"java"}],"slug":"write-a-simple-hello-world-program-in-java","total":0}]}